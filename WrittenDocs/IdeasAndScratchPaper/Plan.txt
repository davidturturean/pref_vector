PLAN

Exploring Cross-Model Latent Direction Alignment for Output Style Preferences

Motivation and Novelty
I’m interested in whether a single latent direction in one language model’s internal space can capture a high-level preference like “concise vs. verbose summaries” – and if so, whether that same vector works without modification in a different model family. Recent research has shown that certain features (for example, sentiment polarity) correspond to linear directions in a model’s activation space

. In other words, one can often find a single vector in the hidden state of a language model that consistently shifts the output along some dimension of behavior (e.g. making text more positive vs. negative). There’s also evidence that we can deliberately learn such “steering vectors” to induce desired behaviors: adding a trained perturbation vector at a model’s hidden layer can reliably evoke coherent, high-level changes in output
. These findings motivate the idea that conciseness or verbosity might likewise be encoded by a specific direction in latent space. The novel question I want to tackle is whether this learned direction is universal enough to carry over between different model architectures. Most existing work sticks to a single model or, if comparing models, assumes you need an explicit mapping or fine-tuning to translate one model’s internal representations into another’s. Indeed, distinct neural networks trained on similar data do tend to learn analogous internal features, but aligning them usually requires at least a linear transformation

. No one (to my knowledge) has tested if a preference-vector from one model will directly work in another model’s latent space without any adaptation. If it does, that suggests a surprising degree of shared structure in how different LLMs represent high-level preferences. If it doesn’t, we still learn something – and we have a fallback plan to introduce a minimal adapter (a small linear projection) to bridge the gap. Either outcome is interesting: success would imply a form of latent space alignment across model families, while a failure (and subsequent rescue by a tiny adapter) would quantify how misaligned these representations are and whether a simple linear map can fix it. This exploration could reveal whether “conciseness vs. verbosity” is a fundamentally similar concept encoded in similar geometric directions inside different models, or if each model’s latent space is uniquely oriented despite performing the same task.

Implementation Plan
To investigate this, I will implement the project in clear stages. The goal is to learn a latent direction for concise vs. verbose output in one model (a smaller open-source LLM like Mistral-7B) and then test it on a different model family (such as Gemma or LLaMA-3). Below are the concrete steps and methods for each stage:
Data Preparation (Summarization with Style Variations): First, I’ll curate or generate a dataset of text summarization tasks that naturally allows for a concise summary and a verbose summary. For each source text (e.g. a news article or story), I will create two target summaries: one very brief (few sentences) and one more detailed. If an existing dataset with multiple summary lengths isn’t readily available, I can generate these by prompting a strong model (or the model itself) to produce both a “short summary” and a “long, detailed summary” for the same input. This gives me paired examples of the same content summarized in different styles. These pairs will serve as training signals to learn the direction of variation (concise ↔ verbose). I’ll ensure the dataset covers a range of topics so the learned vector isn’t tied to just one context.
Learning the Preference Direction in Mistral-7B: Using the above data, I will train a single vector v that, when added to Mistral-7B’s latent state at a certain layer, consistently shifts the model’s output from concise to verbose or vice-versa. A straightforward approach is to operate on the transformer’s residual stream (activation vector) at an intermediate layer. For example, at a chosen layer (say the middle layer of the network), I can insert v into the residual stream during inference as an additive bias. The training procedure might look like:
Freeze all model weights and initialize v (a vector of the same dimension as the model’s hidden state) randomly.
For each training pair (same input, concise output vs. verbose output), run the model twice: once normally (to get the baseline hidden activations and output) and once with the vector v added to the chosen layer’s activations (to get the “steered” output). We want the steered output to match the verbose target when v is added (and conversely match the concise target when v is subtracted, ideally).
Define a loss that measures how close the steered output is to the desired verbose summary (e.g., using a language-model loss or a simpler proxy like comparing output lengths or using an entailment score to ensure the longer summary still covers the content). We can also simultaneously enforce that without the vector, the model remains closer to the concise style, to properly discriminate the effect.
Use gradient descent to adjust v (and only v) to minimize this loss. Essentially, v will learn to nudge the model’s hidden activations in a direction that produces more verbose outputs. This is analogous to learning a “steering vector” for the verbosity preference.
Another approach (if the above proves tricky) is to analytically derive v from differences in hidden states: for each pair, subtract the layer-$L$ activation of the concise run from that of the verbose run, and take an average across many examples to see if a consistent difference emerges. This average difference vector could serve as v. I might try this as a quick check; if the differences align in a certain direction across examples, it’s a good sign that a single vector could work. Regardless of method, the output of this step is a learned direction vector v in Mistral-7B’s layer-$L$ residual space that pushes summaries toward verbosity (and potentially the opposite direction $-v$ pushes towards conciseness).

Validating the Direction on the Source Model: Before moving on, I’ll validate that v indeed works as intended in Mistral-7B. I will take a set of new test prompts and generate summaries with the model under three conditions: no intervention, adding v at layer $L$, and subtracting v at layer $L$. I expect to see a measurable difference in the length and detail of the summaries. For example, with +v, the summaries should contain more words, more details or elaborations, and possibly more secondary points, whereas with -v, the summaries should become very brief or terse. I’ll quantify this by metrics such as average output length (word count or token count), and perhaps a brevity score or a style classifier that can distinguish verbose vs. concise. If possible, I might also do a quick human evaluation on a few cases to ensure the longer summaries still make sense and the shorter ones aren’t missing key content. A successful validation is if v reliably shifts the style in the expected direction without ruining the factual content of the summary. This would confirm that we’ve indeed captured the “verbosity preference” in a single latent direction for Mistral-7B.
Transferring the Vector to a Different Model: Now comes the core experiment – taking the exact same vector v and applying it to a different model family. I’ll choose a model that is architecturally similar enough to allow a meaningful comparison, but different in weights and training lineage. For example, Gemma-13B (a Google model) or LLaMA-3 7B could be good targets, since they are contemporary model families with possibly different training data or tokenizer, but I can pick a model with the same hidden size if possible. Assuming Gemma or LLaMA-3 has a corresponding layer $L$ with the same vector dimension (for instance, many 7B-13B models use 4096-dimensional hidden states), I will inject v into that model’s residual stream at the analogous layer position. This means during Gemma’s inference on a summary prompt, I add v to the activations at layer $L$ (the same relative layer index or one that roughly corresponds by order or functionality). The procedure is: take a set of test summary prompts (the same ones used in step 3 or new ones for generality), run Gemma/LLaMA-3 in three conditions (no vector, +v, -v at layer $L$), and observe the outputs.
Evaluating Cross-Model Effects: I will evaluate whether v causes Gemma or LLaMA-3 to change its output style towards verbosity or conciseness. The evaluation method will mirror what was done in step 3. Key things to measure:
Output length and detail: Does adding v make the other model’s summaries longer on average, or include more detail, compared to its normal output? Does subtracting v make them shorter? I’ll compare average token counts and maybe use a verbosity scorer (like measuring redundancy or number of unique facts included).
Content preservation: It’s important to check that we’re not just making the model ramble or go off-topic. I can use ROUGE or another summary evaluation metric between the model’s output and a reference summary to ensure the main points are still covered when v is applied. If v introduces verbosity effectively, we should see outputs that are longer and still relevant to the input.
Qualitative examples: I’ll also look at a few examples manually. This will help identify if v triggers some odd behavior in the new model (e.g., maybe the new model wasn’t trained the same way and v causes repetition or some different quirk).
If, remarkably, the unaltered vector v works decently in the new model (i.e. it consistently makes outputs more verbose or more concise as intended), that would be a striking result. It would suggest a raw alignment in the latent geometry across these model families – essentially they share a common “direction” for verbosity preference. However, it’s also quite possible that v in Gemma’s space does nothing or has an unpredictable effect, given that the internal basis of the latent space could be rotated or otherwise different from Mistral’s.
Introducing a Linear Adapter (Fallback Strategy): If the direct transfer of v fails to produce the desired effect in the new model (or produces only a weak effect), the next step is to learn a small linear mapping to translate v from Mistral-7B’s space into a vector that works for the target model. The idea is to use a minimal adapter to bridge the representational gap. I will keep this adapter extremely simple – likely a single linear layer (matrix) that takes the original $d$-dimensional vector and outputs a $d$-dimensional vector (since the models’ hidden dimension $d$ is presumably the same or comparable). There are a couple of ways to train this adapter:
By aligning activations: Use a set of parallel inputs on both models and compare their internal activations. For example, run both Mistral and Gemma on the same set of prompts without any steering. Then, add v in Mistral and see what change it causes in Mistral’s layer-$L$ activations or outputs. Now adjust the adapter’s parameters so that when we pass v through it to get v' and add v' to Gemma’s activations, Gemma’s outputs change in the same way (as measured by some loss function) as Mistral’s did. Concretely, if v made Mistral’s summary 30% longer, we’d like the transformed vector $Wv = v'$ to make Gemma’s summary longer by a similar proportion for corresponding prompts. We can optimize $W$ to minimize the discrepancy in effects or to maximize the increase in Gemma’s output length when $v'$ is applied, using the development prompts as a guide.
Directly on output style: Alternatively, optimize the adapter $W$ by gradient descent on Gemma’s outputs: treat $Wv$ as a trainable vector (since $W$ is small) and adjust $W$ to make Gemma produce verbose summaries on a few training examples (similar to step 2, but now $W$ is what we’re learning while $v$ stays fixed). Essentially, we want $W$ such that $Wv$ becomes a good steering vector in Gemma’s latent space. Because $W$ is just a linear map (with relatively few parameters, e.g. $d^2$ if fully dense, though we could constrain it to a diagonal or something to reduce parameters), this is still a lightweight adaptation – much lighter than fine-tuning the whole model. It’s akin to finding the “closest” vector in Gemma’s space to represent the same concept that $v$ represented in Mistral.
In practice, I might start with a simple approach like linear regression: use a handful of example pairs (where we know how $v$ affected Mistral’s output, and we want $Wv$ to achieve similar in Gemma) to solve for $W$. If the hidden dimensions align well, even a diagonal scaling or rotation might do. If not, a more flexible linear layer will be used. The end result of this step will be an adapted vector $v' = Wv$ tailored for the new model.
Testing the Adapted Vector in the New Model: After training the linear adapter, I will repeat the evaluation from step 5 but using the transformed vector $v'$ in the target model. That is, add $v'$ at Gemma/LLaMA-3’s layer $L$ and see if we now observe the intended shift towards verbosity. I’ll use the same metrics – output length increase, detail, and content fidelity – to quantify the effect. Ideally, $v'$ should perform comparably (or better) on the new model as $v$ did on the original. If $v'$ succeeds, it demonstrates that the concept of verbosity preference can be translated with only a tiny amount of learned transformation, supporting the idea that the underlying representational spaces weren’t arbitrarily different but had a linear relationship. I will also verify that without the adapter (i.e. using raw $v$) there was indeed little effect, to contrast the improvement. This serves as a sanity check that the adapter wasn’t unnecessary. If $v'$ still fails to elicit the behavior, that indicates a deeper mismatch between the models’ internal representations than a linear map can fix – a valuable finding in itself, though it would be somewhat surprising given prior evidence that representations of different nets can often be aligned linearly
.
Analysis of Outcomes and Significance: In the final analysis, I will summarize whether the latent direction transferred successfully and what that implies. There are a few possible outcomes:
If the unaltered vector v worked across models, that would suggest an unexpected commonality in how Mistral-7B and Gemma/LLaMA-3 encode the “concise vs. verbose” style preference. We could then speculate that certain abstract features might correspond to roughly the same direction in many transformer language models – perhaps due to convergent training on similar data or architectures. This would be a striking result, hinting at universal latent axes for some behaviors.
If v did not work directly but the adapter-augmented vector v' did, the project would still be a success: it would demonstrate that only a very small transformation (learned in a lightweight way) was needed to bridge the gap. This aligns with the notion that representational differences are mostly linear (a rotation/scale etc.)

. It would also mirror how word embeddings from different languages or models can be aligned with a linear map – here we’d be aligning a feature vector between models. Such a result would show that even if model families organize their internal spaces differently, the underlying concept of verbosity is encoded similarly enough to translate with linear algebra.
If even the adapted vector fails to transfer the behavior, that indicates a more fundamental misalignment of internal features. Perhaps each model develops its own basis for representing preferences, with no simple correspondence. This outcome would be informative by setting a kind of limit on how comparable different LLMs are internally. It might suggest exploring more complex mapping functions or looking into why the geometry diverges.
No matter which outcome, I will document examples and metrics to share with you (my PhD mentor) and the research group. The experiment directly tests an assumption often made in interpretability and transfer learning – that there is some shared structure across models – in a very targeted way. If successful, it opens up the exciting possibility of model-agnostic steering: finding vectors in one model and deploying them broadly, which could save effort in personalization or alignment tasks. If not, we gain a better understanding of the distinct “worlds” each model lives in internally, and we’ll know that cross-model alignment isn’t automatic (but maybe achievable with small adapters). I’m enthusiastic about this project because it blends interpretability with practical transfer learning, and whatever the result, it will shed light on how different large language models might be secretly more similar than we think – or confirm that each is its own alien mind.