Idea

Does a Preference Vector Transfer Between Models?
Motivation: Large LMs implicitly encode many “preference dimensions” - for example, a continuum from concise to verbose style, or from formal to casual tone. If we can isolate a direction in one model’s activation space that represents such a preference, can we re-use that knowledge in a different model? This question is about transferable steerability: instead of fine-tuning each model for a style or preference, we’d like to learn one vector that can be applied to many models to elicit the same behavioral shift. This has practical appeal (quickly aligning new models to certain style/tone without full retraining) and theoretical significance for understanding how similarly or differently models encode high-level preferences. If a “preference vector” is truly general, it would hint that different LLMs learn analogous latent axes for the same concept.
Recent Work: A growing line of research shows that you can find steering vectors in a single model to control behavior. For instance, the Activation Addition ( of Turner et al. (2023) ) method takes a pair of prompts (one exhibiting a target trait and one without it) and computes the difference in their hidden representations; this difference vector can then be added during inference to steer the model’s output towards the target trait. Turner et al. (2023) applied such activation-difference vectors to steer GPT-2 and Llama-2’s behavior, demonstrating the model would, say, become more polite or terse when the appropriate vector is added. Other researchers have extended this to more robust contrastive activation vectors that average over many prompt pairs to get a cleaner direction for a behavior (reducing noise). These techniques confirm that high-level preferences are often encoded linearly in one model’s latent space. There’s even been work on aligning models via representation tweaks: Liu et al. (2024) introduced Representation Alignment from Human Feedback (RAHF), which finds and manipulates internal representations corresponding to human-valued preferences (like truthfulness or toxicity) instead of doing full RLHF fine-tuning. RAHF showed it’s possible to capture broad preferences in a model’s activations and adjust them directly, underscoring the idea of a modifiable preference subspace.
However, transferring these directions across different models is challenging. Within the same model family, results are promising: e.g., a steering vector learned on LLaMA-2 7B base still had a significant effect when applied to LLaMA-2 Chat (which is RLHF-tuned), indicating the aligned model retained a similar latent basis But across different architectures (say, a Meta model vs. a Google model), the correspondence isn’t one-to-one. Recent research suggests that a simple linear transformation can bridge the gap: Huang et al. (2025) found that concept vectors (they call them steering vectors) for a given idea can be mapped between models by learning a linear projection, enabling one model’s “steer” to control another model. In fact, they showed a single linear mapper could align multiple concept representations across models, and even that vectors from smaller LLMs can influence larger LLMs. Without such a mapper, though, a direct copy of a vector from Model A to Model B often fails or behaves inconsistently, because each model’s latent space is oriented differently. In summary, prior work tells us: within-family transfer is partly feasible, but cross-family transfer likely needs an adaptation step.
Proposed Experiment: We will attempt to learn a preference vector in one model and inject it into another with no (or minimal) conversion. Specifically, take Mistral-7B (open, high-quality model) and identify an activation-direction for a clear stylistic preference – for example, “concise answer vs. verbose answer.” We can generate pairs of completions from Mistral: one where we prompt it to be very brief, and one where we prompt it to be very detailed. By differencing the hidden states (using the Activation Addition method or its contrastive variant) at a suitable layer, we’ll extract an activation delta that represents “more verbose” (or conversely “more concise,” depending on the direction). We’ll test that in Mistral itself first: adding the delta to a neutral prompt’s activations should make the output more verbose, and subtracting should make it more curt. Once we have a solid preference vector for Mistral, we’ll freeze it.
Next, we’ll inject this exact same vector into other models at analogous layers to see what happens. For example, we take the delta and add it into LLaMA-3 7B’s residual stream (at the corresponding mid-layer) during generation, and observe if LLaMA-3’s output becomes more verbose. We’d do the same with Gemma-7B, etc. The key is no additional training or fine-tuning on those target models – just a zero-shot transfer of the activation shift. We’ll measure changes in output length, formality, or whichever trait we targeted, perhaps using automatic metrics or GPT-4 evaluations for consistency. This will tell us whether the latent direction for “verbosity” in Mistral means anything to a different model or not. A successful transfer (even partially) would be pretty astonishing, since it implies a shared internal geometry for that preference, however this is likely to fail. We do this mostly to get a feel for what happens when this is attempted nonetheless.
Next, we introduce a lightweight linear adapter to map the vector from one model’s space into the other’s. This could be as simple as a single-layer transformation matrix that we learn by aligning a few known reference vectors (for instance, we could use a small set of prompts to generate a handful of known concept deltas in both models and solve for a linear mapping). This idea is inspired by Huang et al.’s finding that a learned linear transform can align concept representations. In our project scope, we’d train such an adapter on perhaps one or two concept pairs and then apply it to the preference vector we care about. Essentially, we’d be saying: “if the raw Mistral verbosity vector doesn’t steer LLaMA-3, let’s gently rotate it into LLaMA-3’s basis using a learned matrix, then try again.” Even training a small mapper, we avoid full fine-tuning - it’s a surgical transfer of one feature. We’ll evaluate if the adapted vector successfully imparts the preference to the target model (e.g., does LLaMA-3 with the transformed vector now become verbose as desired?).
Novelty & Feasibility: This experiment would be one of the first to demonstrate cross-model preference transfer in practice. Prior work showed it conceptually with simple tasks; here we’d apply it between cutting-edge open models (Mistral, LLaMA-3, etc.) on a real stylistic trait. If it works even moderately, it suggests an exciting universality of representation. If it doesn’t, that result is informative too - it might highlight how differently models learn preferences, or the need for better alignment methods. Either way, we’ll gain insight into the geometry of model internals. Extracting activation differences and testing injections are well-bounded tasks using existing frameworks, and we can leverage open-source libraries for hooking model activations. Training a linear adapter is also lightweight (with small data and quick convergence). In summary, this feels doable and could yield the first evidence of a plug-and-play “preference vector” that generalizes beyond one model family.
